{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/FRI-Energy-Analytics/energyanalytics/blob/main/EA_logo.jpg?raw=true\" width=\"240\" height=\"240\" />\n",
    "</p>\n",
    "\n",
    "# Backpropagation\n",
    "## Freshman Research Initiative Energy Analytics CS 309\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have done the forward passes through our networks, but to train our networks we need to update the weights and bias tensors with backpropagation. We do this by showing the network a bunch of different examples, measuring how far the predictions are from the correct answer, and updating the weights and bias tensors to get closer to the lowest error. This is called gradient descent. To do this, we need to define a loss/cost function. We can use mean squared loss for binary classification:\n",
    "\n",
    "$$\n",
    "\\large \\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of training examples, $y_i$ are the true labels, and $\\hat{y}_i$ are the predicted labels. When we minimize this loss, we optimize the weights and bias tensors so that the network can make correct predictions.\n",
    "\n",
    "To train the weights with gradient descent, we propagate the gradient of the loss backwards through the network. Each operation has some gradient between the inputs and outputs. As we send the gradients backwards, we multiply the incoming gradient with the gradient for the operation. Mathematically, this is really just calculating the gradient of the loss with respect to the weights using the chain rule.\n",
    "\n",
    "$$\n",
    "\\large \\frac{\\partial \\ell}{\\partial W_1} = \\frac{\\partial L_1}{\\partial W_1} \\frac{\\partial S}{\\partial L_1} \\frac{\\partial L_2}{\\partial S} \\frac{\\partial \\ell}{\\partial L_2}\n",
    "$$\n",
    "\n",
    "We update our weights using this gradient with some learning rate $\\alpha$. \n",
    "\n",
    "$$\n",
    "\\large W^\\prime_1 = W_1 - \\alpha \\frac{\\partial \\ell}{\\partial W_1}\n",
    "$$\n",
    "\n",
    "The learning rate $\\alpha$ is set such that the weight update steps are small enough that the iterative method settles in a minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses\n",
    "\n",
    "In the `nn` module there are a bunch of different losses we can use. Today we are going to focus on cross-entropy (`nn.CrossEntropyLoss`). You'll usually see the loss assigned to `criterion`. With a classification problem we're using softmax to predict class probabilities. With a softmax output you want to use cross-entropy as the loss. To calculate the loss, you define the criterion then pass in the output of your network and the correct labels.\n",
    "\n",
    "However, `nn.LogSoftmax()` and `nn.NLLLoss()` are in one single class in PyTorch. This means that we have to pass the output of our network into the loss function, and not the output from the softmax function. This output is often called scores, or *logits*.\n",
    "\n",
    "Before building our network, let's go through our normal data processing that we have seen for several weeks now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn import preprocessing #for label encoding\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'well_data.csv') #read it in\n",
    "le = preprocessing.LabelEncoder()\n",
    "top_names = data.TOP\n",
    "le.fit(data.TOP)\n",
    "tops = le.transform(data.TOP)\n",
    "data.drop('TOP', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.utils.data.TensorDataset(torch.Tensor(np.array(data)), torch.Tensor(np.array(tops)))\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = 64, shuffle = True)\n",
    "\n",
    "features, labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to define our model, let's use `nn.Sequential` today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=27, out_features=54, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=54, out_features=10, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n",
      "tensor(6.8755, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size = 27\n",
    "hidden_sizes = [54, 10]\n",
    "output_size = 2\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                     )\n",
    "print(model)\n",
    "\n",
    "# Define our loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Do a forward pass\n",
    "logps = model(features)\n",
    "\n",
    "# Now we need to calculate our loss (how close are the predictions)\n",
    "loss = criterion(logps, labels.type(torch.LongTensor))\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "Now we have made our forward pass through the network and calculated the loss, we need to perform backpropagation and update our weights and bias tensors. In PyTorch this is done with the `autograd` module. This module automatically calculates the gradients of tensors. It does this by keeping track of operations performed on tensors, then inverting those operations and caclulating gradients. In PyTorch keeping track of operations on tensors is done with the keyword `requires_grad = True` at creation, or `feature.requires_grad_(True)`.\n",
    "\n",
    "To call gradients you just need to use the `.backward` method. We can call this on our loss function. First let's check the gradient on our model weights after the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's call the `.backward` method on our loss, and see what the gradients are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-6.4139e+00, -1.6867e-02, -1.6701e-02,  ...,  6.2831e-03,\n",
      "         -5.9042e-04, -4.6634e-03],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [-1.3922e+01, -3.6611e-02, -3.6251e-02,  ...,  1.3638e-02,\n",
      "         -1.2815e-03, -1.0122e-02],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 4.2147e+01,  1.1084e-01,  1.0975e-01,  ..., -4.1288e-02,\n",
      "          3.8798e-03,  3.0644e-02]])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our backpropagation and gradients in place, but we still need one last piece to update the weights with the gradients. This piece is called an optimizer. In PyTorch it is in the `optim` module. For this exercise, let's use stochastic gradient descent, which takes the model parameters, and a learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting this all together now, to train our network in PyTorch we do the following:\n",
    "\n",
    "1. Complete a forward pass through the network to get our logits\n",
    "2. Use the logits with the actual values to calculate the loss\n",
    "3. Complete a backward pass through the network to calculate the gradients\n",
    "4. Take a step with the optimizer to update the weights\n",
    "\n",
    "Let's take a single step through training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# set the model weights gradient to none\n",
    "model[0].weight.grad = None\n",
    "\n",
    "print(model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-6.4139e+00, -1.6867e-02, -1.6701e-02,  ...,  6.2831e-03,\n",
      "         -5.9042e-04, -4.6634e-03],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [-1.3922e+01, -3.6611e-02, -3.6251e-02,  ...,  1.3638e-02,\n",
      "         -1.2815e-03, -1.0122e-02],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 4.2147e+01,  1.1084e-01,  1.0975e-01,  ..., -4.1288e-02,\n",
      "          3.8798e-03,  3.0644e-02]])\n"
     ]
    }
   ],
   "source": [
    "# Clear the gradients that have accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# 1. Forward pass\n",
    "logps = model(features)\n",
    "\n",
    "# 2. Calculate the loss\n",
    "loss = criterion(logps, labels.type(torch.LongTensor))\n",
    "\n",
    "# 3. Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# 4. Update weights\n",
    "optimizer.step()\n",
    "\n",
    "print(model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a first pass through the network both forwards and backwards updating gradients and taking optimal steps along the way. Put it all together for batches of 64 samples in 10 different epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 16.910012881076614\n",
      "Training loss: 0.39766802066980406\n",
      "Training loss: 0.39912986200909284\n",
      "Training loss: 0.39758059295804\n",
      "Training loss: 0.3971757744980413\n",
      "Training loss: 0.39655751509721887\n",
      "Training loss: 0.39765095502831216\n",
      "Training loss: 0.3970917201665945\n",
      "Training loss: 0.3979684360498606\n",
      "Training loss: 0.3971339514435724\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for features, labels in train_loader:\n",
    "        # clear accumulated gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward\n",
    "        logps = model(features)\n",
    "        # loss\n",
    "        loss = criterion(logps, labels.type(torch.LongTensor))\n",
    "        # backwards pass\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your model is now trained. Let's see how we can make predictions with it on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
