{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/jessepisel/energy_analytics/blob/master/EA_logo.jpg?raw=true\" width=\"220\" height=\"240\" />\n",
    "\n",
    "</p>\n",
    "\n",
    "# Neural Networks in Numpy\n",
    "## Freshman Research Initiative Energy Analytics CS 309\n",
    "\n",
    "#### Jesse Pisel, Assistant Professor of Practice, University of Texas at Austin\n",
    "**[Twitter](http://twitter.com/geologyjesse)** | **[GitHub](https://github.com/jessepisel)** | **[GoogleScholar](https://scholar.google.com/citations?user=Z4JzYgIAAAAJ&hl=en&oi=ao)** | **[LinkedIn](https://www.linkedin.com/in/jesse-pisel-70519430/)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to do our normal import of the data and get the labels encoded like we have for the past few weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEPT</th>\n",
       "      <th>AHT10</th>\n",
       "      <th>AHT20</th>\n",
       "      <th>AHT30</th>\n",
       "      <th>AHT60</th>\n",
       "      <th>AHT90</th>\n",
       "      <th>AHTCO60</th>\n",
       "      <th>AHTCO90</th>\n",
       "      <th>DPHZ</th>\n",
       "      <th>DSOZ</th>\n",
       "      <th>...</th>\n",
       "      <th>ITT</th>\n",
       "      <th>NPOR</th>\n",
       "      <th>PEFZ</th>\n",
       "      <th>RSOZ</th>\n",
       "      <th>RXOZ</th>\n",
       "      <th>SDEV</th>\n",
       "      <th>SP</th>\n",
       "      <th>SPHI</th>\n",
       "      <th>RHOZ</th>\n",
       "      <th>TOP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5466</th>\n",
       "      <td>1914.0</td>\n",
       "      <td>1.6167</td>\n",
       "      <td>3.0335</td>\n",
       "      <td>7.5475</td>\n",
       "      <td>8.5244</td>\n",
       "      <td>9.1691</td>\n",
       "      <td>117.3103</td>\n",
       "      <td>109.0619</td>\n",
       "      <td>-0.4129</td>\n",
       "      <td>0.5048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2649</td>\n",
       "      <td>0.4847</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>0.0348</td>\n",
       "      <td>2.9599</td>\n",
       "      <td>1.0538</td>\n",
       "      <td>1.6250</td>\n",
       "      <td>0.7009</td>\n",
       "      <td>3.3313</td>\n",
       "      <td>MATANUSKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5467</th>\n",
       "      <td>1913.5</td>\n",
       "      <td>1.6164</td>\n",
       "      <td>3.0324</td>\n",
       "      <td>7.5492</td>\n",
       "      <td>8.5195</td>\n",
       "      <td>9.1830</td>\n",
       "      <td>117.3782</td>\n",
       "      <td>108.8963</td>\n",
       "      <td>-0.6763</td>\n",
       "      <td>0.3208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4760</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.7452</td>\n",
       "      <td>1.0770</td>\n",
       "      <td>10.9375</td>\n",
       "      <td>0.6161</td>\n",
       "      <td>3.7659</td>\n",
       "      <td>MATANUSKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5468</th>\n",
       "      <td>1913.0</td>\n",
       "      <td>1.6163</td>\n",
       "      <td>3.0317</td>\n",
       "      <td>7.5488</td>\n",
       "      <td>8.5243</td>\n",
       "      <td>9.1852</td>\n",
       "      <td>117.3116</td>\n",
       "      <td>108.8711</td>\n",
       "      <td>-0.9772</td>\n",
       "      <td>0.2371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2651</td>\n",
       "      <td>0.4754</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3407</td>\n",
       "      <td>1.0509</td>\n",
       "      <td>43.8125</td>\n",
       "      <td>0.5991</td>\n",
       "      <td>4.2624</td>\n",
       "      <td>MATANUSKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5469</th>\n",
       "      <td>1912.5</td>\n",
       "      <td>1.6162</td>\n",
       "      <td>3.0311</td>\n",
       "      <td>7.5493</td>\n",
       "      <td>8.5248</td>\n",
       "      <td>9.1936</td>\n",
       "      <td>117.3051</td>\n",
       "      <td>108.7711</td>\n",
       "      <td>-1.1748</td>\n",
       "      <td>0.2120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2652</td>\n",
       "      <td>0.4853</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2168</td>\n",
       "      <td>0.8236</td>\n",
       "      <td>79.5000</td>\n",
       "      <td>0.6521</td>\n",
       "      <td>4.5884</td>\n",
       "      <td>MATANUSKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5470</th>\n",
       "      <td>1912.0</td>\n",
       "      <td>1.6161</td>\n",
       "      <td>3.0305</td>\n",
       "      <td>7.5496</td>\n",
       "      <td>8.5289</td>\n",
       "      <td>9.1974</td>\n",
       "      <td>117.2483</td>\n",
       "      <td>108.7263</td>\n",
       "      <td>-1.1654</td>\n",
       "      <td>0.2080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2652</td>\n",
       "      <td>0.4471</td>\n",
       "      <td>9.9845</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1797</td>\n",
       "      <td>0.7958</td>\n",
       "      <td>108.5000</td>\n",
       "      <td>0.6699</td>\n",
       "      <td>4.5729</td>\n",
       "      <td>MATANUSKA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        DEPT   AHT10   AHT20   AHT30   AHT60   AHT90   AHTCO60   AHTCO90  \\\n",
       "5466  1914.0  1.6167  3.0335  7.5475  8.5244  9.1691  117.3103  109.0619   \n",
       "5467  1913.5  1.6164  3.0324  7.5492  8.5195  9.1830  117.3782  108.8963   \n",
       "5468  1913.0  1.6163  3.0317  7.5488  8.5243  9.1852  117.3116  108.8711   \n",
       "5469  1912.5  1.6162  3.0311  7.5493  8.5248  9.1936  117.3051  108.7711   \n",
       "5470  1912.0  1.6161  3.0305  7.5496  8.5289  9.1974  117.2483  108.7263   \n",
       "\n",
       "        DPHZ    DSOZ  ...     ITT    NPOR     PEFZ    RSOZ    RXOZ    SDEV  \\\n",
       "5466 -0.4129  0.5048  ...  0.2649  0.4847  10.0000  0.0348  2.9599  1.0538   \n",
       "5467 -0.6763  0.3208  ...  0.2650  0.4760  10.0000  0.0000  1.7452  1.0770   \n",
       "5468 -0.9772  0.2371  ...  0.2651  0.4754  10.0000  0.0000  0.3407  1.0509   \n",
       "5469 -1.1748  0.2120  ...  0.2652  0.4853  10.0000  0.0000  0.2168  0.8236   \n",
       "5470 -1.1654  0.2080  ...  0.2652  0.4471   9.9845  0.0000  0.1797  0.7958   \n",
       "\n",
       "            SP    SPHI    RHOZ        TOP  \n",
       "5466    1.6250  0.7009  3.3313  MATANUSKA  \n",
       "5467   10.9375  0.6161  3.7659  MATANUSKA  \n",
       "5468   43.8125  0.5991  4.2624  MATANUSKA  \n",
       "5469   79.5000  0.6521  4.5884  MATANUSKA  \n",
       "5470  108.5000  0.6699  4.5729  MATANUSKA  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r'well_data.csv') #read it in\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing #for label encoding\n",
    "#label encode our formation data\n",
    "le = preprocessing.LabelEncoder()\n",
    "top_names = data.TOP\n",
    "le.fit(data.TOP)\n",
    "tops = le.transform(data.TOP)\n",
    "data.drop('TOP', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.values, tops, test_size=0.2, random_state=86)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have our data organized and split, now let's define some functions for our network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to figure out what kind of activation function we want to use. Let's go with a sigmoid function of the form:\n",
    "\n",
    "$$\\sigma{(x)}=\\frac{1}{1+e^{-x}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to define a function that calculates our prediction using our weights, bias, and features of the form:\n",
    "\n",
    "$$\\hat{y} = \\sigma(w_1 x_1 + w_2 x_2 + b)$$\n",
    "\n",
    "We can express the RHS as:\n",
    "$$\n",
    "\\sigma(\\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}+b)\n",
    "$$\n",
    "\n",
    "Remember we want to take the dot product of the features and weights, add the bias, then run the entire thing through the activation function. Let's do a forward pass through this layer with random weights for one feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(86)\n",
    "features = X_train[0] #features for one sample\n",
    "r_weights = np.random.rand(features.shape[0])\n",
    "r_bias = np.random.rand(1)\n",
    "y_hat = activation(np.matmul(features, r_weights)+r_bias)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the output for a single neuron. Neurons come into their own when you stack the individual perceptrons into layers and stacks of layers into networks. In the case of a two layer network, the output of one layer becomes the input for the next layer. The network can be expressed as:\n",
    "\n",
    "$$\n",
    "\\hat{y} =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
    "$$\n",
    "\n",
    "where $W_1$ are the weights for the first layer and $W_2$ are the weights for the second layer.\n",
    "Let's calculate the forward pass through a two layer network for all the neurons and one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(86)\n",
    "n_input = X_train.shape[1]\n",
    "n_hidden = 2\n",
    "n_output = 1\n",
    "features = X_train[0]\n",
    "target = y_train[0]\n",
    "# Weights for inputs to hidden layer\n",
    "W1 = np.random.randn(n_input, n_hidden)\n",
    "# Weights for hidden layer to output layer\n",
    "W2 = np.random.randn(n_hidden, n_output)\n",
    "\n",
    "# and bias terms for hidden and output layers\n",
    "B1 = np.random.randn(1, n_hidden)\n",
    "B2 = np.random.randn(1, n_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to multiply our features with our weights, add the bias, and run it through the activation. After that, take the ouput and multiply it with the second weights matrix, and add the second bias. This is our early prediction that needs to be scaled with some kind of activation function. We will do that next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jrp4932\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.66494327]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = activation(np.matmul(features, W1) + B1)\n",
    "y_hat = np.matmul(h, W2) + B2\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's start to build a new network. Since we have 27 features, let's make a network that takes in 27 input units, has 54 hidden units, and has two output units (one for each of our classes). We are going to use our sigmoid activation for the hidden layer of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = X_train.shape[1] #27 features\n",
    "n_hidden = 54\n",
    "n_output = 2\n",
    "features = X_train[0]\n",
    "target = y_train[0]\n",
    "# Weights for inputs to hidden layer\n",
    "W1 = np.random.randn(n_input, n_hidden)\n",
    "# Weights for hidden layer to output layer\n",
    "W2 = np.random.randn(n_hidden, n_output)\n",
    "\n",
    "# and bias terms for hidden and output layers\n",
    "B1 = np.random.randn(1, n_hidden)\n",
    "B2 = np.random.randn(1, n_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jrp4932\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6.19994504, 2.53728256]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = activation(np.matmul(features, W1)+B1)\n",
    "y_hat = np.matmul(h, W2)+B2\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output from the network looks somewhat useful, but it would be more useful if we could calculate a probability distribution from it, so that we could select the class with the highest probability. To do this, we are going to use a softmax function \n",
    "$$\n",
    "\\Large \\sigma(x_i) = \\cfrac{e^{x_i}}{\\sum_k^K{e^{x_k}}}\n",
    "$$\n",
    "\n",
    "What this does is scale each input $x_i$ between 0 and 1 and normalizes the values to give a probability distribution equal to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run our predictions through the softmax function, we get the probability for each class. Here we see that the first class has a 99% probability and the second class has a 1% probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97497807, 0.02502193]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, as we add layers into the network it will get much more challenging to track what is happening at each layer. So to deal with this we are going to use `PyTorch`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
