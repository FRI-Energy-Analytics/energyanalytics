{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/jessepisel/energy_analytics/blob/master/EA_logo.jpg?raw=true\" width=\"220\" height=\"240\" />\n",
    "\n",
    "</p>\n",
    "\n",
    "# Neural Networks in PyTorch Forward Pass\n",
    "## Freshman Research Initiative Energy Analytics CS 309\n",
    "\n",
    "#### Jesse Pisel, Assistant Professor of Practice, University of Texas at Austin\n",
    "**[Twitter](http://twitter.com/geologyjesse)** | **[GitHub](https://github.com/jessepisel)** | **[GoogleScholar](https://scholar.google.com/citations?user=Z4JzYgIAAAAJ&hl=en&oi=ao)** | **[LinkedIn](https://www.linkedin.com/in/jesse-pisel-70519430/)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEPT</th>\n",
       "      <th>AHT10</th>\n",
       "      <th>AHT20</th>\n",
       "      <th>AHT30</th>\n",
       "      <th>AHT60</th>\n",
       "      <th>AHT90</th>\n",
       "      <th>AHTCO60</th>\n",
       "      <th>AHTCO90</th>\n",
       "      <th>DPHZ</th>\n",
       "      <th>DSOZ</th>\n",
       "      <th>...</th>\n",
       "      <th>ITT</th>\n",
       "      <th>NPOR</th>\n",
       "      <th>PEFZ</th>\n",
       "      <th>RSOZ</th>\n",
       "      <th>RXOZ</th>\n",
       "      <th>SDEV</th>\n",
       "      <th>SP</th>\n",
       "      <th>SPHI</th>\n",
       "      <th>RHOZ</th>\n",
       "      <th>TOP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5466</th>\n",
       "      <td>1914.0</td>\n",
       "      <td>1.6167</td>\n",
       "      <td>3.0335</td>\n",
       "      <td>7.5475</td>\n",
       "      <td>8.5244</td>\n",
       "      <td>9.1691</td>\n",
       "      <td>117.3103</td>\n",
       "      <td>109.0619</td>\n",
       "      <td>-0.4129</td>\n",
       "      <td>0.5048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2649</td>\n",
       "      <td>0.4847</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>0.0348</td>\n",
       "      <td>2.9599</td>\n",
       "      <td>1.0538</td>\n",
       "      <td>1.6250</td>\n",
       "      <td>0.7009</td>\n",
       "      <td>3.3313</td>\n",
       "      <td>MATANUSKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5467</th>\n",
       "      <td>1913.5</td>\n",
       "      <td>1.6164</td>\n",
       "      <td>3.0324</td>\n",
       "      <td>7.5492</td>\n",
       "      <td>8.5195</td>\n",
       "      <td>9.1830</td>\n",
       "      <td>117.3782</td>\n",
       "      <td>108.8963</td>\n",
       "      <td>-0.6763</td>\n",
       "      <td>0.3208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4760</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.7452</td>\n",
       "      <td>1.0770</td>\n",
       "      <td>10.9375</td>\n",
       "      <td>0.6161</td>\n",
       "      <td>3.7659</td>\n",
       "      <td>MATANUSKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5468</th>\n",
       "      <td>1913.0</td>\n",
       "      <td>1.6163</td>\n",
       "      <td>3.0317</td>\n",
       "      <td>7.5488</td>\n",
       "      <td>8.5243</td>\n",
       "      <td>9.1852</td>\n",
       "      <td>117.3116</td>\n",
       "      <td>108.8711</td>\n",
       "      <td>-0.9772</td>\n",
       "      <td>0.2371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2651</td>\n",
       "      <td>0.4754</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3407</td>\n",
       "      <td>1.0509</td>\n",
       "      <td>43.8125</td>\n",
       "      <td>0.5991</td>\n",
       "      <td>4.2624</td>\n",
       "      <td>MATANUSKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5469</th>\n",
       "      <td>1912.5</td>\n",
       "      <td>1.6162</td>\n",
       "      <td>3.0311</td>\n",
       "      <td>7.5493</td>\n",
       "      <td>8.5248</td>\n",
       "      <td>9.1936</td>\n",
       "      <td>117.3051</td>\n",
       "      <td>108.7711</td>\n",
       "      <td>-1.1748</td>\n",
       "      <td>0.2120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2652</td>\n",
       "      <td>0.4853</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2168</td>\n",
       "      <td>0.8236</td>\n",
       "      <td>79.5000</td>\n",
       "      <td>0.6521</td>\n",
       "      <td>4.5884</td>\n",
       "      <td>MATANUSKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5470</th>\n",
       "      <td>1912.0</td>\n",
       "      <td>1.6161</td>\n",
       "      <td>3.0305</td>\n",
       "      <td>7.5496</td>\n",
       "      <td>8.5289</td>\n",
       "      <td>9.1974</td>\n",
       "      <td>117.2483</td>\n",
       "      <td>108.7263</td>\n",
       "      <td>-1.1654</td>\n",
       "      <td>0.2080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2652</td>\n",
       "      <td>0.4471</td>\n",
       "      <td>9.9845</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1797</td>\n",
       "      <td>0.7958</td>\n",
       "      <td>108.5000</td>\n",
       "      <td>0.6699</td>\n",
       "      <td>4.5729</td>\n",
       "      <td>MATANUSKA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        DEPT   AHT10   AHT20   AHT30   AHT60   AHT90   AHTCO60   AHTCO90  \\\n",
       "5466  1914.0  1.6167  3.0335  7.5475  8.5244  9.1691  117.3103  109.0619   \n",
       "5467  1913.5  1.6164  3.0324  7.5492  8.5195  9.1830  117.3782  108.8963   \n",
       "5468  1913.0  1.6163  3.0317  7.5488  8.5243  9.1852  117.3116  108.8711   \n",
       "5469  1912.5  1.6162  3.0311  7.5493  8.5248  9.1936  117.3051  108.7711   \n",
       "5470  1912.0  1.6161  3.0305  7.5496  8.5289  9.1974  117.2483  108.7263   \n",
       "\n",
       "        DPHZ    DSOZ  ...     ITT    NPOR     PEFZ    RSOZ    RXOZ    SDEV  \\\n",
       "5466 -0.4129  0.5048  ...  0.2649  0.4847  10.0000  0.0348  2.9599  1.0538   \n",
       "5467 -0.6763  0.3208  ...  0.2650  0.4760  10.0000  0.0000  1.7452  1.0770   \n",
       "5468 -0.9772  0.2371  ...  0.2651  0.4754  10.0000  0.0000  0.3407  1.0509   \n",
       "5469 -1.1748  0.2120  ...  0.2652  0.4853  10.0000  0.0000  0.2168  0.8236   \n",
       "5470 -1.1654  0.2080  ...  0.2652  0.4471   9.9845  0.0000  0.1797  0.7958   \n",
       "\n",
       "            SP    SPHI    RHOZ        TOP  \n",
       "5466    1.6250  0.7009  3.3313  MATANUSKA  \n",
       "5467   10.9375  0.6161  3.7659  MATANUSKA  \n",
       "5468   43.8125  0.5991  4.2624  MATANUSKA  \n",
       "5469   79.5000  0.6521  4.5884  MATANUSKA  \n",
       "5470  108.5000  0.6699  4.5729  MATANUSKA  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r'well_data.csv') #read it in\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing #for label encoding\n",
    "#label encode our formation data\n",
    "le = preprocessing.LabelEncoder()\n",
    "top_names = data.TOP\n",
    "le.fit(data.TOP)\n",
    "tops = le.transform(data.TOP)\n",
    "data.drop('TOP', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have our data organized and split, now let's get into `PyTorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PyTorch` is just like `Numpy` in that we can do things like matrix multiplications and manipulate `tensors` which are the `PyToch` equivalent of `arrays`. In fact we can create a `PyTorch` `tensor` directly from a `Numpy` `array`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.6220e+03,  4.4251e+00,  4.5191e+00,  ...,  1.7500e+00,\n",
       "          3.6780e-01,  2.4403e+00],\n",
       "        [ 2.6905e+03,  1.2609e+01,  1.3061e+01,  ..., -8.6875e+00,\n",
       "          2.4510e-01,  2.4811e+00],\n",
       "        [ 4.0075e+03,  7.6512e+00,  7.6816e+00,  ...,  2.4375e+00,\n",
       "          3.0040e-01,  2.4709e+00],\n",
       "        ...,\n",
       "        [ 3.8820e+03,  3.2788e+00,  3.3559e+00,  ...,  0.0000e+00,\n",
       "          4.5630e-01,  2.4266e+00],\n",
       "        [ 4.2270e+03,  1.1139e+01,  1.1348e+01,  ..., -1.0438e+01,\n",
       "          2.9420e-01,  2.4057e+00],\n",
       "        [ 3.1425e+03,  1.3644e+01,  1.4071e+01,  ..., -3.9375e+00,\n",
       "          2.1720e-01,  2.5147e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instead of doing a test, train split we are going to use torch's dataloader\n",
    "# here we take our features and labels, convert them to numpy arrays, and then convert them to tensors\n",
    "\n",
    "train = torch.utils.data.TensorDataset(torch.Tensor(np.array(data)), torch.Tensor(np.array(tops)))\n",
    "\n",
    "# Now we split the data into different batch sizes\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = 64, shuffle = True)\n",
    "# Here we grab some of the features and labels\n",
    "features, labels = next(iter(train_loader))\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do pretty much everything that `Numpy` does in `PyTorch`. Let's go ahead and reimplement our network from the previous notebook in `PyTorch`. Start with a sigmoid activation function like we had in the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    return 1 / (1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's start to build our network. Since we have 27 features, let's make a network that takes in 27 input units, has 54 hidden units, and has two output units. We are going to use our sigmoid activation for the hidden layer of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = features.shape[1] #27 features\n",
    "n_hidden = 54\n",
    "n_output = 2\n",
    "\n",
    "# Weights for inputs to hidden layer\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "# Weights for hidden layer to output layer\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# and bias terms for hidden and output layers\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "B2 = torch.randn((1, n_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.2210,  1.3566],\n",
      "        [-3.2209,  1.3564],\n",
      "        [-3.2209,  1.3564],\n",
      "        [-3.3356,  1.7754],\n",
      "        [-3.2209,  1.3564],\n",
      "        [-3.2209,  1.3564],\n",
      "        [-3.3345,  0.7197],\n",
      "        [-3.2209,  1.3564],\n",
      "        [-3.2209,  1.3564],\n",
      "        [-3.3356,  1.7754]])\n"
     ]
    }
   ],
   "source": [
    "hidden = activation(torch.mm(features, W1)+B1)\n",
    "y_hat = torch.mm(hidden, W2)+B2\n",
    "print(y_hat[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the last notebook, we want to run our final output through a softmax function to get a probability for each class. Let's define a softmax function using `PyTorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run our predictions through the softmax function, we get the probability for each class. Here we see that the first class has a high probability and the second class has a low probability for the first 10 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0102, 0.9898],\n",
      "        [0.0102, 0.9898],\n",
      "        [0.0102, 0.9898],\n",
      "        [0.0060, 0.9940],\n",
      "        [0.0102, 0.9898],\n",
      "        [0.0102, 0.9898],\n",
      "        [0.0171, 0.9829],\n",
      "        [0.0102, 0.9898],\n",
      "        [0.0102, 0.9898],\n",
      "        [0.0060, 0.9940]])\n"
     ]
    }
   ],
   "source": [
    "print(softmax(y_hat)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can all get wildly complex, but luckily `PyTorch` has lots of modules that makes it much easier to build and train neural networks. Let's use the `nn` module to build the same network from above and see how it does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        # we are inheriting from the nn.Module class\n",
    "        # This along with the line below creates a class that tracks architecture\n",
    "        # and has those methods and attributes\n",
    "        super().__init__() \n",
    "        \n",
    "        # Inputs to hidden layer linear transformation\n",
    "        # The weights and bias tensors are automatically created\n",
    "        self.hidden = nn.Linear(27, 54)\n",
    "        # Output layer, 2 units, one for each class\n",
    "        # same with weights and bias tensor for the output layer\n",
    "        self.output = nn.Linear(54, 2)\n",
    "        \n",
    "        # Define sigmoid activation and softmax output \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "        # nn.Module networks need to have a forward method defined\n",
    "        # this method takes a tensor and passes it through the operations above\n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden): Linear(in_features=27, out_features=54, bias=True)\n",
       "  (output): Linear(in_features=54, out_features=2, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax(dim=0)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the network and look at its text representation\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our architecture let's go ahead and make a forward pass through the network and see what the output is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0169, 0.0174],\n",
      "        [0.0155, 0.0154],\n",
      "        [0.0155, 0.0154],\n",
      "        [0.0171, 0.0176],\n",
      "        [0.0156, 0.0154],\n",
      "        [0.0155, 0.0154],\n",
      "        [0.0155, 0.0153],\n",
      "        [0.0155, 0.0154],\n",
      "        [0.0140, 0.0138],\n",
      "        [0.0168, 0.0172]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(model.forward(features.float())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both classes have similar probability. This is because the network has not been trained and the random weights and bias tensors yield a random prediction centered between the two classes.\n",
    "\n",
    "### Building Networks\n",
    "\n",
    "There are lots of ways we can build networks in `PyTorch`, let's take a look at a couple of different options. First, we can use the `nn.functional` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Inputs to hidden layer \n",
    "        self.hidden = nn.Linear(27, 54)\n",
    "        # Output layer, 2 units, same as before\n",
    "        self.output = nn.Linear(54, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Hidden layer with sigmoid activation\n",
    "        x = torch.sigmoid(self.hidden(x))\n",
    "        # Output layer with softmax activation\n",
    "        x = F.softmax(self.output(x), dim=0)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0152, 0.0156],\n",
      "        [0.0169, 0.0161],\n",
      "        [0.0151, 0.0154],\n",
      "        [0.0152, 0.0156],\n",
      "        [0.0152, 0.0156],\n",
      "        [0.0172, 0.0162],\n",
      "        [0.0159, 0.0144],\n",
      "        [0.0151, 0.0164],\n",
      "        [0.0142, 0.0139],\n",
      "        [0.0150, 0.0160]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the network and look at its text representation\n",
    "model = Network()\n",
    "model\n",
    "print(model.forward(features.float())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add in different activation functions such as `ReLu`. Let's add another hidden layer to our network and use `ReLu` activation functions for the hidden layers, and `Softmax` for our output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=27, out_features=54, bias=True)\n",
       "  (fc2): Linear(in_features=54, out_features=10, bias=True)\n",
       "  (fc3): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Defining the layers, 27, 54, 10 units each\n",
    "        self.fc1 = nn.Linear(27, 54)\n",
    "        self.fc2 = nn.Linear(54, 10)\n",
    "        # Output layer, 2 units, same as above\n",
    "        self.fc3 = nn.Linear(10, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Forward pass through the network, returns the output logits '''\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the weights and bias tensors for the different layers in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0103, -0.1655,  0.0881,  ..., -0.0273, -0.1077,  0.1087],\n",
      "        [ 0.1207,  0.1368,  0.0889,  ..., -0.0606,  0.0185, -0.0042],\n",
      "        [ 0.0782,  0.0809,  0.0131,  ..., -0.0685,  0.0523,  0.0028],\n",
      "        ...,\n",
      "        [-0.0340, -0.0925, -0.1813,  ..., -0.1811, -0.1805, -0.1083],\n",
      "        [-0.1005, -0.1281,  0.1265,  ..., -0.1028,  0.0426,  0.0050],\n",
      "        [-0.0429, -0.1316, -0.1778,  ...,  0.0416, -0.1582,  0.1541]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0979,  0.0906,  0.1698, -0.1031, -0.0441, -0.0693,  0.1183, -0.1859,\n",
      "         0.1809,  0.0882,  0.0707,  0.0408, -0.0953,  0.0376, -0.1632,  0.1893,\n",
      "        -0.1736, -0.1135, -0.1874, -0.1073, -0.0306, -0.0889, -0.0358,  0.0191,\n",
      "         0.1872, -0.0086, -0.0286, -0.0331, -0.0768,  0.0154,  0.0049, -0.1875,\n",
      "         0.0699,  0.1010, -0.1181, -0.1818,  0.1758,  0.0943,  0.0851, -0.0296,\n",
      "        -0.0445,  0.0213,  0.1443,  0.1306, -0.0102, -0.1580,  0.1033,  0.1626,\n",
      "         0.1302, -0.0478,  0.0506,  0.0257, -0.0951, -0.1789],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.fc1.weight)\n",
    "print(model.fc1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential networks\n",
    "We can also define networks using `nn.Sequential` where we define the input size, hidden layer sizes, and the output size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=27, out_features=54, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=54, out_features=10, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (5): Softmax(dim=0)\n",
      ")\n",
      "tensor([[3.4407e-09, 5.4438e-09],\n",
      "        [4.6929e-18, 2.3776e-05],\n",
      "        [2.8010e-06, 6.7094e-12],\n",
      "        [1.8913e-06, 9.9627e-10],\n",
      "        [1.5728e-09, 2.3568e-09],\n",
      "        [4.0881e-20, 2.6142e-04],\n",
      "        [3.4388e-23, 2.7660e-02],\n",
      "        [3.2906e-14, 4.2045e-07],\n",
      "        [1.1566e-04, 5.4128e-13],\n",
      "        [4.5798e-23, 1.2102e-01]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size = 27\n",
    "hidden_sizes = [54, 10]\n",
    "output_size = 2\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=0))\n",
    "print(model)\n",
    "\n",
    "# Forward pass through the network and display output\n",
    "print(model.forward(features.float())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have built the forward pass through the network, but how do we update the weights and biases? We do this through training and backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
